{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü´Å TB Detection with Proven AST - FIXED VERSION\n",
    "\n",
    "**Uses the EXACT same AST code that achieved 88.98% energy savings on malaria!**\n",
    "\n",
    "This notebook:\n",
    "- ‚úÖ Properly handles TBX11K dataset structure\n",
    "- ‚úÖ Uses proven `train_ast.py` from Malaria project\n",
    "- ‚úÖ Expected: 85-90% energy savings + 90%+ accuracy\n",
    "\n",
    "---\n",
    "\n",
    "**‚öôÔ∏è Setup**: Runtime ‚Üí Change runtime type ‚Üí GPU (T4 recommended)\n",
    "\n",
    "**‚è±Ô∏è Time**: ~2-3 hours with GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Clone Malaria Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/oluwafemidiakhoa/Malaria.git\n",
    "%cd Malaria\n",
    "!git pull origin main\n",
    "\n",
    "print(\"‚úÖ Malaria project cloned!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Setup Kaggle API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import os\n",
    "\n",
    "print(\"üìÅ Upload your kaggle.json:\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "!mkdir -p ~/.kaggle\n",
    "!cp kaggle.json ~/.kaggle/\n",
    "!chmod 600 ~/.kaggle/kaggle.json\n",
    "\n",
    "print(\"‚úÖ Kaggle API configured!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q torch torchvision timm adaptive-sparse-training>=1.0.1 \\\n",
    "    scikit-learn matplotlib seaborn pyyaml tqdm kaggle pillow numpy\n",
    "\n",
    "print(\"‚úÖ All dependencies installed!\")\n",
    "\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"\\nüñ•Ô∏è GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è No GPU detected!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Download TBX11K Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle datasets download -d usmanshams/tbx-11\n",
    "!unzip -q tbx-11.zip -d tb_data\n",
    "\n",
    "print(\"‚úÖ TB dataset downloaded!\")\n",
    "print(\"\\nüìÅ Exploring dataset structure...\")\n",
    "!find tb_data -type d | head -20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Explore TBX11K Structure (IMPORTANT!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Explore the actual structure\n",
    "tb_root = Path('tb_data')\n",
    "\n",
    "print(\"üìä TBX11K Dataset Structure:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Find all directories with images\n",
    "for item in sorted(tb_root.rglob('*')):\n",
    "    if item.is_dir():\n",
    "        png_count = len(list(item.glob('*.png')))\n",
    "        jpg_count = len(list(item.glob('*.jpg')))\n",
    "        total = png_count + jpg_count\n",
    "        if total > 0:\n",
    "            rel_path = item.relative_to(tb_root)\n",
    "            print(f\"  {rel_path}: {total} images\")\n",
    "\n",
    "# Count all images\n",
    "all_images = list(tb_root.rglob('*.png')) + list(tb_root.rglob('*.jpg'))\n",
    "print(f\"\\nüìà Total images found: {len(all_images)}\")\n",
    "\n",
    "# Sample some paths to understand labeling\n",
    "print(f\"\\nüìù Sample image paths:\")\n",
    "for img in all_images[:10]:\n",
    "    print(f\"  {img.relative_to(tb_root)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Smart Data Organization (Handles TBX11K structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "tb_root = Path('tb_data')\n",
    "all_images = list(tb_root.rglob('*.png')) + list(tb_root.rglob('*.jpg'))\n",
    "\n",
    "print(f\"üîç Analyzing {len(all_images)} images...\\n\")\n",
    "\n",
    "# Smart classification based on TBX11K structure\n",
    "data = []\n",
    "for img_path in all_images:\n",
    "    path_lower = str(img_path).lower()\n",
    "    parts = [p.lower() for p in img_path.parts]\n",
    "    \n",
    "    # TBX11K structure detection\n",
    "    # Common patterns: 'Tuberculosis', 'Normal', 'Sick', 'Healthy'\n",
    "    is_tb = False\n",
    "    is_normal = False\n",
    "    \n",
    "    # Check directory names\n",
    "    for part in parts:\n",
    "        if 'tuberculosis' in part or 'tb' == part or 'sick' in part or 'abnormal' in part:\n",
    "            is_tb = True\n",
    "        if 'normal' in part or 'healthy' in part:\n",
    "            is_normal = True\n",
    "    \n",
    "    # Assign label\n",
    "    if is_tb and not is_normal:\n",
    "        label = 'TB'\n",
    "    elif is_normal and not is_tb:\n",
    "        label = 'Normal'\n",
    "    else:\n",
    "        # Ambiguous - try filename\n",
    "        fname_lower = img_path.name.lower()\n",
    "        if 'normal' in fname_lower or 'healthy' in fname_lower:\n",
    "            label = 'Normal'\n",
    "        elif 'tb' in fname_lower or 'sick' in fname_lower:\n",
    "            label = 'TB'\n",
    "        else:\n",
    "            # Skip if we can't determine\n",
    "            continue\n",
    "    \n",
    "    data.append((img_path, label))\n",
    "\n",
    "# Check distribution\n",
    "label_counts = Counter([d[1] for d in data])\n",
    "print(\"üìä Label distribution:\")\n",
    "for label, count in label_counts.items():\n",
    "    print(f\"  {label}: {count:,} ({count/len(data)*100:.1f}%)\")\n",
    "\n",
    "# Verify we have both classes\n",
    "if len(label_counts) < 2:\n",
    "    print(\"\\n‚ùå ERROR: Only one class found!\")\n",
    "    print(\"\\nTBX11K appears to be single-class. Let's check the structure:\")\n",
    "    print(\"\\nüìÇ Directory tree:\")\n",
    "    !ls -R tb_data/TBX11K/ | head -100\n",
    "    \n",
    "    print(\"\\n‚ö†Ô∏è SOLUTION: We need to use a different dataset with both Normal and TB classes.\")\n",
    "    print(\"Try: 'tawsifurrahman/tuberculosis-tb-chest-xray-dataset' instead.\")\n",
    "else:\n",
    "    # We have both classes - proceed with split\n",
    "    print(f\"\\n‚úÖ Found {len(data)} usable images with both classes!\")\n",
    "    \n",
    "    # Split into train/val (80/20)\n",
    "    train_data, val_data = train_test_split(\n",
    "        data, test_size=0.2, random_state=42, \n",
    "        stratify=[d[1] for d in data]\n",
    "    )\n",
    "    \n",
    "    # Create directory structure\n",
    "    print(\"\\nüìÅ Creating data directories...\")\n",
    "    for split, split_data in [('train', train_data), ('val', val_data)]:\n",
    "        for label in ['Normal', 'TB']:\n",
    "            dest = Path(f'data/{split}/{label}')\n",
    "            dest.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        for img_path, label in split_data:\n",
    "            dest_path = Path(f'data/{split}/{label}/{img_path.name}')\n",
    "            if not dest_path.exists():  # Avoid duplicates\n",
    "                shutil.copy(img_path, dest_path)\n",
    "    \n",
    "    print(\"\\n‚úÖ Data organized:\")\n",
    "    print(f\"\\n   Train: {len(train_data):,} images\")\n",
    "    for label in ['Normal', 'TB']:\n",
    "        count = len(list(Path(f'data/train/{label}').glob('*')))\n",
    "        print(f\"      {label}: {count:,}\")\n",
    "    \n",
    "    print(f\"\\n   Val: {len(val_data):,} images\")\n",
    "    for label in ['Normal', 'TB']:\n",
    "        count = len(list(Path(f'data/val/{label}').glob('*')))\n",
    "        print(f\"      {label}: {count:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6B: Alternative - Use Better TB Dataset\n",
    "\n",
    "**If TBX11K doesn't have both classes, use this instead:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALTERNATIVE DATASET with confirmed Normal + TB classes\n",
    "# Only run this if Step 6 failed\n",
    "\n",
    "# Clean up previous download\n",
    "!rm -rf tb_data\n",
    "!rm -f *.zip\n",
    "\n",
    "# Download alternative TB dataset\n",
    "!kaggle datasets download -d tawsifurrahman/tuberculosis-tb-chest-xray-dataset\n",
    "!unzip -q tuberculosis-tb-chest-xray-dataset.zip -d tb_data\n",
    "\n",
    "print(\"‚úÖ Alternative TB dataset downloaded!\")\n",
    "print(\"\\nüìÅ Dataset structure:\")\n",
    "!ls -la tb_data/\n",
    "\n",
    "# This dataset has clear Normal/ and Tuberculosis/ folders\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "# Find images in Normal and TB folders\n",
    "tb_root = Path('tb_data')\n",
    "data = []\n",
    "\n",
    "# Look for Normal images\n",
    "for normal_dir in tb_root.rglob('Normal'):\n",
    "    if normal_dir.is_dir():\n",
    "        for img in normal_dir.glob('*.png'):\n",
    "            data.append((img, 'Normal'))\n",
    "        for img in normal_dir.glob('*.jpg'):\n",
    "            data.append((img, 'Normal'))\n",
    "\n",
    "# Look for TB images\n",
    "for tb_dir in tb_root.rglob('Tuberculosis'):\n",
    "    if tb_dir.is_dir():\n",
    "        for img in tb_dir.glob('*.png'):\n",
    "            data.append((img, 'TB'))\n",
    "        for img in tb_dir.glob('*.jpg'):\n",
    "            data.append((img, 'TB'))\n",
    "\n",
    "from collections import Counter\n",
    "label_counts = Counter([d[1] for d in data])\n",
    "print(f\"\\nüìä Label distribution:\")\n",
    "for label, count in label_counts.items():\n",
    "    print(f\"  {label}: {count:,}\")\n",
    "\n",
    "if len(label_counts) == 2:\n",
    "    # Split and organize\n",
    "    train_data, val_data = train_test_split(\n",
    "        data, test_size=0.2, random_state=42, \n",
    "        stratify=[d[1] for d in data]\n",
    "    )\n",
    "    \n",
    "    for split, split_data in [('train', train_data), ('val', val_data)]:\n",
    "        for label in ['Normal', 'TB']:\n",
    "            dest = Path(f'data/{split}/{label}')\n",
    "            dest.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        for img_path, label in split_data:\n",
    "            dest_path = Path(f'data/{split}/{label}/{img_path.name}')\n",
    "            shutil.copy(img_path, dest_path)\n",
    "    \n",
    "    print(\"\\n‚úÖ Data organized successfully!\")\n",
    "    print(f\"   Train: {len(train_data):,} | Val: {len(val_data):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Create TB Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from pathlib import Path\n",
    "\n",
    "config = {\n",
    "    \"model_name\": \"efficientnet_b0\",\n",
    "    \"num_classes\": 2,\n",
    "    \"image_size\": 224,\n",
    "    \"epochs\": 50,\n",
    "    \"batch_size\": 32,\n",
    "    \"learning_rate\": 0.0003,\n",
    "    \"weight_decay\": 0.0001,\n",
    "    \"num_workers\": 2,\n",
    "    \"amp\": True,\n",
    "    \"train_dir\": \"data/train\",\n",
    "    \"val_dir\": \"data/val\",\n",
    "    \"save_dir\": \"checkpoints_tb_ast\",\n",
    "    \"resume\": True,\n",
    "    \"patience\": 15,\n",
    "    # AST settings - EXACT same as malaria\n",
    "    \"ast_target_activation_rate\": 0.40,\n",
    "    \"ast_initial_threshold\": 3.0,\n",
    "    \"ast_adapt_kp\": 0.005,\n",
    "    \"ast_adapt_ki\": 0.0001,\n",
    "    \"ast_ema_alpha\": 0.1,\n",
    "    \"ast_warmup_epochs\": 2,\n",
    "}\n",
    "\n",
    "config_path = Path(\"configs/config_tb_ast.yaml\")\n",
    "config_path.parent.mkdir(exist_ok=True)\n",
    "\n",
    "with open(config_path, \"w\") as f:\n",
    "    yaml.dump(config, f, default_flow_style=False)\n",
    "\n",
    "print(f\"‚úÖ Config created!\")\n",
    "print(f\"\\n‚öôÔ∏è AST Settings:\")\n",
    "print(f\"  Activation rate: {config['ast_target_activation_rate']*100:.0f}%\")\n",
    "print(f\"  Expected savings: ~{(1-config['ast_target_activation_rate'])*100:.0f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "!mkdir -p '/content/drive/MyDrive/TB_AST_Results'\n",
    "print(\"‚úÖ Drive mounted!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Train TB Model with AST\n",
    "\n",
    "**This will now work with proper data organization!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify data exists before training\n",
    "!ls -la data/train/\n",
    "!ls -la data/val/\n",
    "\n",
    "# Train with proven AST code\n",
    "!python train_ast.py --config configs/config_tb_ast.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: View Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "metrics = []\n",
    "with open('checkpoints_tb_ast/metrics_ast.jsonl', 'r') as f:\n",
    "    for line in f:\n",
    "        metrics.append(json.loads(line))\n",
    "\n",
    "df = pd.DataFrame(metrics)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"üéâ TB DETECTION TRAINING COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "best_acc = df['val_acc'].max() * 100\n",
    "best_epoch = df.loc[df['val_acc'].idxmax(), 'epoch']\n",
    "print(f\"\\nüéØ Best Accuracy: {best_acc:.2f}% (Epoch {best_epoch})\")\n",
    "\n",
    "non_warmup = df[df['epoch'] > 2]\n",
    "if len(non_warmup) > 0:\n",
    "    avg_savings = non_warmup['energy_savings'].mean()\n",
    "    avg_activation = non_warmup['activation_rate'].mean()\n",
    "    print(f\"\\n‚ö° Energy Efficiency:\")\n",
    "    print(f\"   Average Energy Savings: {avg_savings:.1f}%\")\n",
    "    print(f\"   Average Activation Rate: {avg_activation*100:.1f}%\")\n",
    "\n",
    "print(\"\\nüìä Last 10 Epochs:\")\n",
    "print(df[['epoch', 'val_acc', 'activation_rate', 'energy_savings']].tail(10))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"üé§ Results: '{best_acc:.1f}% TB detection with {avg_savings:.0f}% energy savings'\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Save to Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp -r checkpoints_tb_ast /content/drive/MyDrive/TB_AST_Results/\n",
    "!cp configs/config_tb_ast.yaml /content/drive/MyDrive/TB_AST_Results/\n",
    "\n",
    "print(\"‚úÖ Results saved to Google Drive!\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
