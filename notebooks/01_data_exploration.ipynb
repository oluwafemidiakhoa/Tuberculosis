{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìä TBX11K Dataset Exploration\n",
    "\n",
    "**Goal**: Understand the TBX11K chest X-ray dataset structure, distribution, and characteristics.\n",
    "\n",
    "---\n",
    "\n",
    "## Dataset Overview\n",
    "\n",
    "- **Total Images**: 11,200 chest X-rays\n",
    "- **Classes**: Healthy, Sick (non-TB), Active TB, Latent TB, Uncertain\n",
    "- **Resolution**: 512x512 pixels\n",
    "- **Format**: PNG/JPEG\n",
    "- **Annotations**: Bounding boxes for TB regions\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Dataset Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "data_dir = Path('../data/raw')\n",
    "\n",
    "# Check if dataset exists\n",
    "if not data_dir.exists():\n",
    "    print(\"‚ùå Dataset not found. Please download TBX11K dataset first.\")\n",
    "else:\n",
    "    print(f\"‚úÖ Dataset directory found: {data_dir.absolute()}\")\n",
    "    \n",
    "# List contents\n",
    "print(\"\\nDataset contents:\")\n",
    "for item in data_dir.iterdir():\n",
    "    print(f\"  - {item.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Analyze Class Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load metadata (adjust path based on actual dataset structure)\n",
    "# This will vary depending on how TBX11K is organized\n",
    "\n",
    "# Example: If metadata is in CSV\n",
    "# metadata_file = data_dir / 'labels.csv'\n",
    "# df = pd.read_csv(metadata_file)\n",
    "\n",
    "# For now, let's explore the directory structure\n",
    "all_images = list(data_dir.rglob('*.png')) + list(data_dir.rglob('*.jpg')) + list(data_dir.rglob('*.jpeg'))\n",
    "\n",
    "print(f\"Total images found: {len(all_images)}\")\n",
    "print(f\"\\nSample image paths:\")\n",
    "for img_path in all_images[:5]:\n",
    "    print(f\"  {img_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract classes from folder structure (if organized by class)\n",
    "class_counts = Counter([img.parent.name for img in all_images])\n",
    "\n",
    "print(\"Class distribution:\")\n",
    "for class_name, count in class_counts.most_common():\n",
    "    print(f\"  {class_name}: {count} images ({count/len(all_images)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize class distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "classes = list(class_counts.keys())\n",
    "counts = list(class_counts.values())\n",
    "\n",
    "plt.bar(classes, counts, color='steelblue', alpha=0.8)\n",
    "plt.xlabel('Class', fontsize=12)\n",
    "plt.ylabel('Number of Images', fontsize=12)\n",
    "plt.title('TBX11K Dataset - Class Distribution', fontsize=14, fontweight='bold')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "for i, (cls, cnt) in enumerate(zip(classes, counts)):\n",
    "    plt.text(i, cnt + 50, f'{cnt}\\n({cnt/len(all_images)*100:.1f}%)', \n",
    "             ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Analyze Image Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample images for analysis (to avoid loading all 11K images)\n",
    "sample_size = min(500, len(all_images))\n",
    "sample_images = np.random.choice(all_images, sample_size, replace=False)\n",
    "\n",
    "image_stats = []\n",
    "\n",
    "print(f\"Analyzing {sample_size} sample images...\")\n",
    "for img_path in sample_images:\n",
    "    try:\n",
    "        img = Image.open(img_path)\n",
    "        image_stats.append({\n",
    "            'width': img.width,\n",
    "            'height': img.height,\n",
    "            'mode': img.mode,\n",
    "            'size_mb': os.path.getsize(img_path) / (1024*1024)\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {img_path}: {e}\")\n",
    "\n",
    "stats_df = pd.DataFrame(image_stats)\n",
    "print(\"\\n‚úÖ Image analysis complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display statistics\n",
    "print(\"Image Statistics:\")\n",
    "print(f\"\\nResolution:\")\n",
    "print(stats_df[['width', 'height']].describe())\n",
    "\n",
    "print(f\"\\nImage Modes:\")\n",
    "print(stats_df['mode'].value_counts())\n",
    "\n",
    "print(f\"\\nFile Sizes:\")\n",
    "print(stats_df['size_mb'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize image size distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Resolution distribution\n",
    "axes[0].scatter(stats_df['width'], stats_df['height'], alpha=0.5, color='steelblue')\n",
    "axes[0].set_xlabel('Width (pixels)')\n",
    "axes[0].set_ylabel('Height (pixels)')\n",
    "axes[0].set_title('Image Resolution Distribution')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# File size distribution\n",
    "axes[1].hist(stats_df['size_mb'], bins=30, color='coral', alpha=0.7)\n",
    "axes[1].set_xlabel('File Size (MB)')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('File Size Distribution')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualize Sample Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample images from each class\n",
    "samples_per_class = 3\n",
    "\n",
    "fig, axes = plt.subplots(len(classes), samples_per_class, figsize=(15, len(classes) * 4))\n",
    "\n",
    "if len(classes) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for i, class_name in enumerate(classes):\n",
    "    class_images = [img for img in all_images if img.parent.name == class_name]\n",
    "    samples = np.random.choice(class_images, min(samples_per_class, len(class_images)), replace=False)\n",
    "    \n",
    "    for j, img_path in enumerate(samples):\n",
    "        img = Image.open(img_path)\n",
    "        axes[i][j].imshow(img, cmap='gray' if img.mode == 'L' else None)\n",
    "        axes[i][j].axis('off')\n",
    "        axes[i][j].set_title(f'{class_name}\\n{img.size[0]}x{img.size[1]}', fontsize=10)\n",
    "\n",
    "plt.suptitle('Sample Chest X-rays from Each Class', fontsize=16, fontweight='bold', y=1.0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Pixel Intensity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze pixel intensity distributions\n",
    "sample_for_intensity = np.random.choice(all_images, min(100, len(all_images)), replace=False)\n",
    "\n",
    "intensity_distributions = {}\n",
    "\n",
    "for img_path in sample_for_intensity:\n",
    "    class_name = img_path.parent.name\n",
    "    img = Image.open(img_path).convert('L')  # Convert to grayscale\n",
    "    img_array = np.array(img)\n",
    "    \n",
    "    if class_name not in intensity_distributions:\n",
    "        intensity_distributions[class_name] = []\n",
    "    \n",
    "    intensity_distributions[class_name].extend(img_array.flatten())\n",
    "\n",
    "print(\"‚úÖ Pixel intensity analysis complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot intensity distributions by class\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "for class_name, intensities in intensity_distributions.items():\n",
    "    plt.hist(intensities, bins=50, alpha=0.5, label=class_name, density=True)\n",
    "\n",
    "plt.xlabel('Pixel Intensity (0-255)', fontsize=12)\n",
    "plt.ylabel('Density', fontsize=12)\n",
    "plt.title('Pixel Intensity Distribution by Class', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create Train/Val/Test Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Create dataframe with all images\n",
    "data = []\n",
    "for img_path in all_images:\n",
    "    data.append({\n",
    "        'image_path': str(img_path),\n",
    "        'class': img_path.parent.name\n",
    "    })\n",
    "\n",
    "df_all = pd.DataFrame(data)\n",
    "print(f\"Total images in dataset: {len(df_all)}\")\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(df_all['class'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split: 70% train, 15% val, 15% test\n",
    "# Stratified split to maintain class distribution\n",
    "\n",
    "train_df, temp_df = train_test_split(\n",
    "    df_all, \n",
    "    test_size=0.3, \n",
    "    stratify=df_all['class'], \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "val_df, test_df = train_test_split(\n",
    "    temp_df, \n",
    "    test_size=0.5, \n",
    "    stratify=temp_df['class'], \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Train set: {len(train_df)} images ({len(train_df)/len(df_all)*100:.1f}%)\")\n",
    "print(f\"Validation set: {len(val_df)} images ({len(val_df)/len(df_all)*100:.1f}%)\")\n",
    "print(f\"Test set: {len(test_df)} images ({len(test_df)/len(df_all)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify stratification\n",
    "print(\"\\nClass distribution across splits:\")\n",
    "print(\"\\nTrain:\")\n",
    "print(train_df['class'].value_counts())\n",
    "print(\"\\nValidation:\")\n",
    "print(val_df['class'].value_counts())\n",
    "print(\"\\nTest:\")\n",
    "print(test_df['class'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save splits to CSV\n",
    "splits_dir = Path('../data/splits')\n",
    "splits_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "train_df.to_csv(splits_dir / 'train.csv', index=False)\n",
    "val_df.to_csv(splits_dir / 'val.csv', index=False)\n",
    "test_df.to_csv(splits_dir / 'test.csv', index=False)\n",
    "\n",
    "print(f\"‚úÖ Splits saved to {splits_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary & Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"DATASET SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nTotal Images: {len(df_all):,}\")\n",
    "print(f\"\\nClasses: {len(classes)}\")\n",
    "for cls in classes:\n",
    "    count = len(df_all[df_all['class'] == cls])\n",
    "    print(f\"  - {cls}: {count:,} ({count/len(df_all)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nImage Properties:\")\n",
    "print(f\"  - Average resolution: {stats_df['width'].mean():.0f}x{stats_df['height'].mean():.0f}\")\n",
    "print(f\"  - Average file size: {stats_df['size_mb'].mean():.2f} MB\")\n",
    "print(f\"  - Image mode: {stats_df['mode'].mode()[0]}\")\n",
    "\n",
    "print(f\"\\nData Splits:\")\n",
    "print(f\"  - Train: {len(train_df):,} images\")\n",
    "print(f\"  - Validation: {len(val_df):,} images\")\n",
    "print(f\"  - Test: {len(test_df):,} images\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"NEXT STEPS\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\n1. ‚úÖ Data exploration complete\")\n",
    "print(\"2. üìù Run preprocessing notebook (02_preprocessing.ipynb)\")\n",
    "print(\"3. üß† Train baseline model (03_baseline_model.ipynb)\")\n",
    "print(\"4. ‚ö° Apply AST training (04_ast_training.ipynb)\")\n",
    "print(\"5. üöÄ Build Gradio demo\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
